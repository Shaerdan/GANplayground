{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the transforms to be applied to the images\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()] # Normalize with mean and std\n",
    ")\n",
    "\n",
    "# Load the training dataset\n",
    "batch_size = 32\n",
    "trainset = datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# Create a data loader for the training dataset\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# Get one or two images from the training set\n",
    "image, label = trainset[0]\n",
    "print(image.shape)\n",
    "image = image.permute(1, 2, 0)\n",
    "\n",
    "# Visualize the image\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title('Label: {}'.format(label))\n",
    "plt.show()\n",
    "        \n",
    "image_size_x = 32\n",
    "image_size_y = 32 \n",
    "num_channels = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_datasize = image_size_x*image_size_y*num_channels # a sample in the trainset is a square image of 32*32, and 3 color channels.\n",
    "input_noise_size = 100;\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Downsample layers\n",
    "        self.conv1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Upsample layers\n",
    "        self.up5 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv5 = DoubleConv(512, 256)\n",
    "        self.up6 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv6 = DoubleConv(256, 128)\n",
    "        self.up7 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv7 = DoubleConv(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.out = nn.Conv2d(64, in_channels, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Downsample\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.pool1(x1)\n",
    "        #print(f\"x2 is {x2.shape}\")\n",
    "        x2 = self.conv2(x2)\n",
    "        x3 = self.pool2(x2)\n",
    "        x3 = self.conv3(x3)\n",
    "        x4 = self.pool3(x3)\n",
    "        x4 = self.conv4(x4)  \n",
    "        #print(f\"x4 is {x4.shape}\")\n",
    "              \n",
    "        # Upsample\n",
    "        x5 = self.up5(x4)\n",
    "        #print(f\"x5 is {x5.shape}\")\n",
    "        #print(f\"x3 is {x3.shape}\")        \n",
    "        x5 = torch.cat([x5, x3], dim=1)\n",
    "        x5 = self.conv5(x5)\n",
    "        x6 = self.up6(x5)\n",
    "        x6 = torch.cat([x6, x2], dim=1)\n",
    "        x6 = self.conv6(x6)\n",
    "        x7 = self.up7(x6)\n",
    "        x7 = torch.cat([x7, x1], dim=1)\n",
    "        x7 = self.conv7(x7)\n",
    "        \n",
    "        # Output\n",
    "        out = self.out(x7)\n",
    "        return out\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(num_channels, 32, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 2, 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, 2, 1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, 2, 1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, 3, 2, 1)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.lrelu(x)\n",
    "        #print(f\"output of conv5 and lrelu is {x.shape}\")\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        #print(f\"input of fc1 after view is {x.shape}\")        \n",
    "        x = self.fc1(x)\n",
    "        x = self.lrelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "#criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizers\n",
    "lr_dis = 0.0002\n",
    "lr_gen = 0.00008\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# define the models:\n",
    "generator = UNet()\n",
    "discriminator = Discriminator(num_channels=3)\n",
    "\n",
    "# Define the loss function and optimizer for the discriminator\n",
    "criterion_D = nn.BCELoss()\n",
    "\n",
    "# Define the loss function and optimizer for the generator\n",
    "criterion_G = nn.BCELoss()\n",
    "\n",
    "# Create the generator optimizer\n",
    "optimizer_G = torch.optim.SGD(generator.parameters(), lr=lr_gen)\n",
    "\n",
    "# Create the discriminator optimizer\n",
    "optimizer_D = torch.optim.SGD(discriminator.parameters(), lr=lr_dis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug chk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shaer\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 Epoch 0: D Loss = 1.3865363597869873, G Loss = 0.7016167044639587 \n",
      "Iteration 2 Epoch 0: D Loss = 1.3864442110061646, G Loss = 0.7014806270599365 \n",
      "Iteration 4 Epoch 0: D Loss = 1.386470079421997, G Loss = 0.701613187789917 \n",
      "Iteration 6 Epoch 0: D Loss = 1.3865196704864502, G Loss = 0.7014679908752441 \n",
      "Iteration 8 Epoch 0: D Loss = 1.386468529701233, G Loss = 0.7015267014503479 \n",
      "Iteration 10 Epoch 0: D Loss = 1.3864080905914307, G Loss = 0.7015517950057983 \n",
      "Iteration 12 Epoch 0: D Loss = 1.3864784240722656, G Loss = 0.7014808654785156 \n",
      "Iteration 14 Epoch 0: D Loss = 1.3865439891815186, G Loss = 0.7015515565872192 \n",
      "Iteration 16 Epoch 0: D Loss = 1.3864054679870605, G Loss = 0.7015645503997803 \n",
      "Iteration 18 Epoch 0: D Loss = 1.3864498138427734, G Loss = 0.7015494108200073 \n",
      "Iteration 20 Epoch 0: D Loss = 1.3864428997039795, G Loss = 0.7015794515609741 \n",
      "Iteration 22 Epoch 0: D Loss = 1.3865115642547607, G Loss = 0.7015438675880432 \n",
      "Iteration 24 Epoch 0: D Loss = 1.3864541053771973, G Loss = 0.701545774936676 \n",
      "Iteration 26 Epoch 0: D Loss = 1.3864917755126953, G Loss = 0.7015437483787537 \n",
      "Iteration 28 Epoch 0: D Loss = 1.38637113571167, G Loss = 0.7015330195426941 \n",
      "Iteration 30 Epoch 0: D Loss = 1.3865115642547607, G Loss = 0.7016763091087341 \n",
      "Iteration 32 Epoch 0: D Loss = 1.3864855766296387, G Loss = 0.7014915347099304 \n",
      "Iteration 34 Epoch 0: D Loss = 1.3864974975585938, G Loss = 0.7016225457191467 \n",
      "Iteration 36 Epoch 0: D Loss = 1.3864405155181885, G Loss = 0.7015126943588257 \n",
      "Iteration 38 Epoch 0: D Loss = 1.3864469528198242, G Loss = 0.7015165090560913 \n",
      "Iteration 40 Epoch 0: D Loss = 1.38649582862854, G Loss = 0.7014991044998169 \n",
      "Iteration 42 Epoch 0: D Loss = 1.3865058422088623, G Loss = 0.7014870643615723 \n",
      "Iteration 44 Epoch 0: D Loss = 1.386474609375, G Loss = 0.701499342918396 \n",
      "Iteration 46 Epoch 0: D Loss = 1.3865044116973877, G Loss = 0.7014846205711365 \n",
      "Iteration 48 Epoch 0: D Loss = 1.3863561153411865, G Loss = 0.7014821767807007 \n",
      "Iteration 50 Epoch 0: D Loss = 1.3864004611968994, G Loss = 0.7014405727386475 \n",
      "Iteration 52 Epoch 0: D Loss = 1.3863420486450195, G Loss = 0.7015873193740845 \n",
      "Iteration 54 Epoch 0: D Loss = 1.386385202407837, G Loss = 0.7013903260231018 \n",
      "Iteration 56 Epoch 0: D Loss = 1.3864226341247559, G Loss = 0.7015088200569153 \n",
      "Iteration 58 Epoch 0: D Loss = 1.3863850831985474, G Loss = 0.7015644311904907 \n",
      "Iteration 60 Epoch 0: D Loss = 1.3864073753356934, G Loss = 0.7015379071235657 \n",
      "Iteration 62 Epoch 0: D Loss = 1.3864121437072754, G Loss = 0.7015417814254761 \n",
      "Iteration 64 Epoch 0: D Loss = 1.3864123821258545, G Loss = 0.7016089558601379 \n",
      "Iteration 66 Epoch 0: D Loss = 1.3863693475723267, G Loss = 0.7014946937561035 \n",
      "Iteration 68 Epoch 0: D Loss = 1.3863749504089355, G Loss = 0.7015568017959595 \n",
      "Iteration 70 Epoch 0: D Loss = 1.3862791061401367, G Loss = 0.7014623284339905 \n",
      "Iteration 72 Epoch 0: D Loss = 1.3864774703979492, G Loss = 0.7014404535293579 \n",
      "Iteration 74 Epoch 0: D Loss = 1.3863967657089233, G Loss = 0.701498806476593 \n",
      "Iteration 76 Epoch 0: D Loss = 1.386433482170105, G Loss = 0.7013912796974182 \n",
      "Iteration 78 Epoch 0: D Loss = 1.3864508867263794, G Loss = 0.7015067934989929 \n",
      "Iteration 80 Epoch 0: D Loss = 1.3863322734832764, G Loss = 0.701438307762146 \n",
      "Iteration 82 Epoch 0: D Loss = 1.3863366842269897, G Loss = 0.7015723586082458 \n",
      "Iteration 84 Epoch 0: D Loss = 1.3863766193389893, G Loss = 0.701483964920044 \n",
      "Iteration 86 Epoch 0: D Loss = 1.3865559101104736, G Loss = 0.7014142870903015 \n",
      "Iteration 88 Epoch 0: D Loss = 1.386453628540039, G Loss = 0.7015539407730103 \n",
      "Iteration 90 Epoch 0: D Loss = 1.3863295316696167, G Loss = 0.7014719843864441 \n",
      "Iteration 92 Epoch 0: D Loss = 1.3863158226013184, G Loss = 0.7014276385307312 \n",
      "Iteration 94 Epoch 0: D Loss = 1.3864829540252686, G Loss = 0.7013128995895386 \n",
      "Iteration 96 Epoch 0: D Loss = 1.3863918781280518, G Loss = 0.7013993859291077 \n",
      "Iteration 98 Epoch 0: D Loss = 1.3863519430160522, G Loss = 0.7013956904411316 \n",
      "Iteration 100 Epoch 0: D Loss = 1.386421799659729, G Loss = 0.70152747631073 \n",
      "Iteration 102 Epoch 0: D Loss = 1.3861829042434692, G Loss = 0.7014542818069458 \n",
      "Iteration 104 Epoch 0: D Loss = 1.3863999843597412, G Loss = 0.7014526128768921 \n",
      "Iteration 106 Epoch 0: D Loss = 1.3864978551864624, G Loss = 0.7015478610992432 \n",
      "Iteration 108 Epoch 0: D Loss = 1.3863725662231445, G Loss = 0.7013794183731079 \n",
      "Iteration 110 Epoch 0: D Loss = 1.3863656520843506, G Loss = 0.7014975547790527 \n",
      "Iteration 112 Epoch 0: D Loss = 1.3862643241882324, G Loss = 0.7014302015304565 \n",
      "Iteration 114 Epoch 0: D Loss = 1.3863584995269775, G Loss = 0.7014647722244263 \n",
      "Iteration 116 Epoch 0: D Loss = 1.3865079879760742, G Loss = 0.7014200687408447 \n",
      "Iteration 118 Epoch 0: D Loss = 1.386426329612732, G Loss = 0.7013911604881287 \n",
      "Iteration 120 Epoch 0: D Loss = 1.386423110961914, G Loss = 0.7015151977539062 \n",
      "Iteration 122 Epoch 0: D Loss = 1.3863972425460815, G Loss = 0.7013957500457764 \n",
      "Iteration 124 Epoch 0: D Loss = 1.3863447904586792, G Loss = 0.7014256119728088 \n",
      "Iteration 126 Epoch 0: D Loss = 1.386260747909546, G Loss = 0.7013734579086304 \n",
      "Iteration 128 Epoch 0: D Loss = 1.3863778114318848, G Loss = 0.7014843821525574 \n",
      "Iteration 130 Epoch 0: D Loss = 1.3862946033477783, G Loss = 0.701377272605896 \n",
      "Iteration 132 Epoch 0: D Loss = 1.386282205581665, G Loss = 0.70152348279953 \n",
      "Iteration 134 Epoch 0: D Loss = 1.3862558603286743, G Loss = 0.7014507055282593 \n",
      "Iteration 136 Epoch 0: D Loss = 1.386185884475708, G Loss = 0.7014131546020508 \n",
      "Iteration 138 Epoch 0: D Loss = 1.386367678642273, G Loss = 0.701432466506958 \n",
      "Iteration 140 Epoch 0: D Loss = 1.386244535446167, G Loss = 0.701503574848175 \n",
      "Iteration 142 Epoch 0: D Loss = 1.3861663341522217, G Loss = 0.7014176845550537 \n",
      "Iteration 144 Epoch 0: D Loss = 1.3862426280975342, G Loss = 0.7014720439910889 \n",
      "Iteration 146 Epoch 0: D Loss = 1.3863332271575928, G Loss = 0.7013958692550659 \n",
      "Iteration 148 Epoch 0: D Loss = 1.3864028453826904, G Loss = 0.7014799118041992 \n",
      "Iteration 150 Epoch 0: D Loss = 1.3864126205444336, G Loss = 0.7013838887214661 \n",
      "Iteration 152 Epoch 0: D Loss = 1.3863189220428467, G Loss = 0.7014369964599609 \n",
      "Iteration 154 Epoch 0: D Loss = 1.3862264156341553, G Loss = 0.7014331221580505 \n",
      "Iteration 156 Epoch 0: D Loss = 1.3862922191619873, G Loss = 0.7013354301452637 \n",
      "Iteration 158 Epoch 0: D Loss = 1.3862210512161255, G Loss = 0.7013823390007019 \n",
      "Iteration 160 Epoch 0: D Loss = 1.3862528800964355, G Loss = 0.7013884782791138 \n",
      "Iteration 162 Epoch 0: D Loss = 1.3863492012023926, G Loss = 0.7013978958129883 \n",
      "Iteration 164 Epoch 0: D Loss = 1.3863697052001953, G Loss = 0.7014427781105042 \n",
      "Iteration 166 Epoch 0: D Loss = 1.3862853050231934, G Loss = 0.7013928294181824 \n",
      "Iteration 168 Epoch 0: D Loss = 1.3863234519958496, G Loss = 0.7012991905212402 \n",
      "Iteration 170 Epoch 0: D Loss = 1.386293888092041, G Loss = 0.7015510201454163 \n",
      "Iteration 172 Epoch 0: D Loss = 1.3862321376800537, G Loss = 0.7013987898826599 \n",
      "Iteration 174 Epoch 0: D Loss = 1.386277437210083, G Loss = 0.7013227343559265 \n",
      "Iteration 176 Epoch 0: D Loss = 1.3863317966461182, G Loss = 0.7013976573944092 \n",
      "Iteration 178 Epoch 0: D Loss = 1.3862988948822021, G Loss = 0.7013182640075684 \n",
      "Iteration 180 Epoch 0: D Loss = 1.3861868381500244, G Loss = 0.7015171051025391 \n",
      "Iteration 182 Epoch 0: D Loss = 1.3861929178237915, G Loss = 0.7013222575187683 \n",
      "Iteration 184 Epoch 0: D Loss = 1.3863017559051514, G Loss = 0.7013389468193054 \n",
      "Iteration 186 Epoch 0: D Loss = 1.3863749504089355, G Loss = 0.7013034820556641 \n",
      "Iteration 188 Epoch 0: D Loss = 1.3862590789794922, G Loss = 0.7014046907424927 \n",
      "Iteration 190 Epoch 0: D Loss = 1.3863950967788696, G Loss = 0.7014495134353638 \n",
      "Iteration 192 Epoch 0: D Loss = 1.3862987756729126, G Loss = 0.7014382481575012 \n",
      "Iteration 194 Epoch 0: D Loss = 1.3861989974975586, G Loss = 0.7012898921966553 \n",
      "Iteration 196 Epoch 0: D Loss = 1.386256217956543, G Loss = 0.7013261318206787 \n",
      "Iteration 198 Epoch 0: D Loss = 1.386268138885498, G Loss = 0.7013292908668518 \n",
      "Iteration 200 Epoch 0: D Loss = 1.3862247467041016, G Loss = 0.7014060616493225 \n",
      "Iteration 202 Epoch 0: D Loss = 1.3862932920455933, G Loss = 0.7013533711433411 \n",
      "Iteration 204 Epoch 0: D Loss = 1.3861935138702393, G Loss = 0.7013227939605713 \n",
      "Iteration 206 Epoch 0: D Loss = 1.3862168788909912, G Loss = 0.7013701796531677 \n",
      "Iteration 208 Epoch 0: D Loss = 1.3862779140472412, G Loss = 0.7014746069908142 \n",
      "Iteration 210 Epoch 0: D Loss = 1.3862426280975342, G Loss = 0.701379656791687 \n",
      "Iteration 212 Epoch 0: D Loss = 1.3862890005111694, G Loss = 0.7014177441596985 \n",
      "Iteration 214 Epoch 0: D Loss = 1.386326789855957, G Loss = 0.7013918161392212 \n",
      "Iteration 216 Epoch 0: D Loss = 1.3863120079040527, G Loss = 0.7012748122215271 \n",
      "Iteration 218 Epoch 0: D Loss = 1.3861523866653442, G Loss = 0.7013657093048096 \n",
      "Iteration 220 Epoch 0: D Loss = 1.3861899375915527, G Loss = 0.7013888359069824 \n",
      "Iteration 222 Epoch 0: D Loss = 1.3863048553466797, G Loss = 0.7013846039772034 \n",
      "Iteration 224 Epoch 0: D Loss = 1.3861327171325684, G Loss = 0.701094388961792 \n",
      "Iteration 226 Epoch 0: D Loss = 1.386195421218872, G Loss = 0.7012998461723328 \n",
      "Iteration 228 Epoch 0: D Loss = 1.386199712753296, G Loss = 0.7012578845024109 \n",
      "Iteration 230 Epoch 0: D Loss = 1.386246681213379, G Loss = 0.701287031173706 \n",
      "Iteration 232 Epoch 0: D Loss = 1.3861644268035889, G Loss = 0.7012198567390442 \n",
      "Iteration 234 Epoch 0: D Loss = 1.3863763809204102, G Loss = 0.7012454271316528 \n",
      "Iteration 236 Epoch 0: D Loss = 1.386200189590454, G Loss = 0.7013174891471863 \n",
      "Iteration 238 Epoch 0: D Loss = 1.3861970901489258, G Loss = 0.7012791633605957 \n",
      "Iteration 240 Epoch 0: D Loss = 1.3862358331680298, G Loss = 0.7011910676956177 \n",
      "Iteration 242 Epoch 0: D Loss = 1.386147379875183, G Loss = 0.7013237476348877 \n",
      "Iteration 244 Epoch 0: D Loss = 1.3862863779067993, G Loss = 0.7014197111129761 \n",
      "Iteration 246 Epoch 0: D Loss = 1.3862009048461914, G Loss = 0.7014452219009399 \n",
      "Iteration 248 Epoch 0: D Loss = 1.3863117694854736, G Loss = 0.7014300227165222 \n",
      "Iteration 250 Epoch 0: D Loss = 1.3861706256866455, G Loss = 0.7012351155281067 \n",
      "Iteration 252 Epoch 0: D Loss = 1.3861804008483887, G Loss = 0.7012892961502075 \n",
      "Iteration 254 Epoch 0: D Loss = 1.3861579895019531, G Loss = 0.7013390064239502 \n",
      "Iteration 256 Epoch 0: D Loss = 1.38631010055542, G Loss = 0.7013192176818848 \n",
      "Iteration 258 Epoch 0: D Loss = 1.3861510753631592, G Loss = 0.7013448476791382 \n",
      "Iteration 260 Epoch 0: D Loss = 1.3861829042434692, G Loss = 0.7013254165649414 \n",
      "Iteration 262 Epoch 0: D Loss = 1.3861654996871948, G Loss = 0.7013388276100159 \n",
      "Iteration 264 Epoch 0: D Loss = 1.3861663341522217, G Loss = 0.7013846039772034 \n",
      "Iteration 266 Epoch 0: D Loss = 1.386170744895935, G Loss = 0.7012592554092407 \n",
      "Iteration 268 Epoch 0: D Loss = 1.3861167430877686, G Loss = 0.7012627124786377 \n",
      "Iteration 270 Epoch 0: D Loss = 1.3861111402511597, G Loss = 0.7013872861862183 \n",
      "Iteration 272 Epoch 0: D Loss = 1.386183261871338, G Loss = 0.7013448476791382 \n",
      "Iteration 274 Epoch 0: D Loss = 1.3861520290374756, G Loss = 0.7012186646461487 \n",
      "Iteration 276 Epoch 0: D Loss = 1.3861284255981445, G Loss = 0.7013272643089294 \n",
      "Iteration 278 Epoch 0: D Loss = 1.3860843181610107, G Loss = 0.7012994885444641 \n",
      "Iteration 280 Epoch 0: D Loss = 1.3861877918243408, G Loss = 0.7012093663215637 \n",
      "Iteration 282 Epoch 0: D Loss = 1.3860599994659424, G Loss = 0.7012637853622437 \n",
      "Iteration 284 Epoch 0: D Loss = 1.3861241340637207, G Loss = 0.7013471126556396 \n",
      "Iteration 286 Epoch 0: D Loss = 1.3862016201019287, G Loss = 0.7012953758239746 \n",
      "Iteration 288 Epoch 0: D Loss = 1.386041283607483, G Loss = 0.7013579607009888 \n",
      "Iteration 290 Epoch 0: D Loss = 1.3861750364303589, G Loss = 0.7011938095092773 \n",
      "Iteration 292 Epoch 0: D Loss = 1.386090874671936, G Loss = 0.7012629508972168 \n",
      "Iteration 294 Epoch 0: D Loss = 1.3861627578735352, G Loss = 0.7013643383979797 \n",
      "Iteration 296 Epoch 0: D Loss = 1.3860766887664795, G Loss = 0.7012845873832703 \n",
      "Iteration 298 Epoch 0: D Loss = 1.3862874507904053, G Loss = 0.7011816501617432 \n",
      "Iteration 300 Epoch 0: D Loss = 1.3860780000686646, G Loss = 0.7012584209442139 \n",
      "Iteration 302 Epoch 0: D Loss = 1.3860950469970703, G Loss = 0.701359748840332 \n",
      "Iteration 304 Epoch 0: D Loss = 1.3861600160598755, G Loss = 0.7012920379638672 \n",
      "Iteration 306 Epoch 0: D Loss = 1.3861182928085327, G Loss = 0.7012262344360352 \n",
      "Iteration 308 Epoch 0: D Loss = 1.3862388134002686, G Loss = 0.7012830376625061 \n",
      "Iteration 310 Epoch 0: D Loss = 1.3861253261566162, G Loss = 0.701245129108429 \n",
      "Iteration 312 Epoch 0: D Loss = 1.3862396478652954, G Loss = 0.701250433921814 \n",
      "Iteration 314 Epoch 0: D Loss = 1.3860896825790405, G Loss = 0.7013385891914368 \n",
      "Iteration 316 Epoch 0: D Loss = 1.3861396312713623, G Loss = 0.7013090252876282 \n",
      "Iteration 318 Epoch 0: D Loss = 1.386123538017273, G Loss = 0.7012680172920227 \n",
      "Iteration 320 Epoch 0: D Loss = 1.386193037033081, G Loss = 0.7011627554893494 \n",
      "Iteration 322 Epoch 0: D Loss = 1.3860247135162354, G Loss = 0.7012306451797485 \n",
      "Iteration 324 Epoch 0: D Loss = 1.3860865831375122, G Loss = 0.701239824295044 \n",
      "Iteration 326 Epoch 0: D Loss = 1.3861727714538574, G Loss = 0.7012239694595337 \n",
      "Iteration 328 Epoch 0: D Loss = 1.3859915733337402, G Loss = 0.7012136578559875 \n",
      "Iteration 330 Epoch 0: D Loss = 1.3862066268920898, G Loss = 0.7012889981269836 \n",
      "Iteration 332 Epoch 0: D Loss = 1.3860089778900146, G Loss = 0.701260507106781 \n",
      "Iteration 334 Epoch 0: D Loss = 1.3860061168670654, G Loss = 0.7011780738830566 \n",
      "Iteration 336 Epoch 0: D Loss = 1.386141061782837, G Loss = 0.7011946439743042 \n",
      "Iteration 338 Epoch 0: D Loss = 1.386233925819397, G Loss = 0.7012131810188293 \n",
      "Iteration 340 Epoch 0: D Loss = 1.3861520290374756, G Loss = 0.7012306451797485 \n",
      "Iteration 342 Epoch 0: D Loss = 1.3861169815063477, G Loss = 0.7012543082237244 \n",
      "Iteration 344 Epoch 0: D Loss = 1.3860806226730347, G Loss = 0.701216459274292 \n",
      "Iteration 346 Epoch 0: D Loss = 1.3860920667648315, G Loss = 0.7012647390365601 \n",
      "Iteration 348 Epoch 0: D Loss = 1.3861287832260132, G Loss = 0.7012643814086914 \n",
      "Iteration 350 Epoch 0: D Loss = 1.3860604763031006, G Loss = 0.7012824416160583 \n",
      "Iteration 352 Epoch 0: D Loss = 1.3861514329910278, G Loss = 0.7012473940849304 \n",
      "Iteration 354 Epoch 0: D Loss = 1.3862197399139404, G Loss = 0.7012009620666504 \n",
      "Iteration 356 Epoch 0: D Loss = 1.3860642910003662, G Loss = 0.701187252998352 \n",
      "Iteration 358 Epoch 0: D Loss = 1.3859386444091797, G Loss = 0.7011328339576721 \n",
      "Iteration 360 Epoch 0: D Loss = 1.3862175941467285, G Loss = 0.7012317180633545 \n",
      "Iteration 362 Epoch 0: D Loss = 1.3861708641052246, G Loss = 0.7011409401893616 \n",
      "Iteration 364 Epoch 0: D Loss = 1.3861587047576904, G Loss = 0.7010666131973267 \n",
      "Iteration 366 Epoch 0: D Loss = 1.3860807418823242, G Loss = 0.7012875080108643 \n",
      "Iteration 368 Epoch 0: D Loss = 1.3860573768615723, G Loss = 0.7012226581573486 \n",
      "Iteration 370 Epoch 0: D Loss = 1.3860316276550293, G Loss = 0.7012383341789246 \n",
      "Iteration 372 Epoch 0: D Loss = 1.3861281871795654, G Loss = 0.7011907696723938 \n",
      "Iteration 374 Epoch 0: D Loss = 1.3860619068145752, G Loss = 0.701204776763916 \n",
      "Iteration 376 Epoch 0: D Loss = 1.3860654830932617, G Loss = 0.7011027932167053 \n",
      "Iteration 378 Epoch 0: D Loss = 1.386103630065918, G Loss = 0.7012297511100769 \n",
      "Iteration 380 Epoch 0: D Loss = 1.385994553565979, G Loss = 0.7011262774467468 \n",
      "Iteration 382 Epoch 0: D Loss = 1.3860899209976196, G Loss = 0.7011586427688599 \n",
      "Iteration 384 Epoch 0: D Loss = 1.3860260248184204, G Loss = 0.7012835144996643 \n",
      "Iteration 386 Epoch 0: D Loss = 1.3860797882080078, G Loss = 0.7010723352432251 \n",
      "Iteration 388 Epoch 0: D Loss = 1.3861351013183594, G Loss = 0.7012162208557129 \n",
      "Iteration 390 Epoch 0: D Loss = 1.38604736328125, G Loss = 0.7011772990226746 \n",
      "Iteration 392 Epoch 0: D Loss = 1.3860797882080078, G Loss = 0.7012971639633179 \n",
      "Iteration 394 Epoch 0: D Loss = 1.3859891891479492, G Loss = 0.7011333107948303 \n",
      "Iteration 396 Epoch 0: D Loss = 1.3861653804779053, G Loss = 0.7011584043502808 \n",
      "Iteration 398 Epoch 0: D Loss = 1.3860986232757568, G Loss = 0.7011964321136475 \n",
      "Iteration 400 Epoch 0: D Loss = 1.3860949277877808, G Loss = 0.7011618614196777 \n",
      "Iteration 402 Epoch 0: D Loss = 1.3859949111938477, G Loss = 0.7011423110961914 \n",
      "Iteration 404 Epoch 0: D Loss = 1.3861171007156372, G Loss = 0.7012315988540649 \n",
      "Iteration 406 Epoch 0: D Loss = 1.3861442804336548, G Loss = 0.7011783123016357 \n",
      "Iteration 408 Epoch 0: D Loss = 1.3860571384429932, G Loss = 0.7011153101921082 \n",
      "Iteration 410 Epoch 0: D Loss = 1.3860011100769043, G Loss = 0.7011310458183289 \n",
      "Iteration 412 Epoch 0: D Loss = 1.3859857320785522, G Loss = 0.7011376023292542 \n",
      "Iteration 414 Epoch 0: D Loss = 1.386009931564331, G Loss = 0.7011703848838806 \n",
      "Iteration 416 Epoch 0: D Loss = 1.386059045791626, G Loss = 0.7012176513671875 \n",
      "Iteration 418 Epoch 0: D Loss = 1.3858846426010132, G Loss = 0.7011738419532776 \n",
      "Iteration 420 Epoch 0: D Loss = 1.3860710859298706, G Loss = 0.7010787725448608 \n",
      "Iteration 422 Epoch 0: D Loss = 1.386167287826538, G Loss = 0.7011355757713318 \n",
      "Iteration 424 Epoch 0: D Loss = 1.3859992027282715, G Loss = 0.7011591196060181 \n",
      "Iteration 426 Epoch 0: D Loss = 1.3858247995376587, G Loss = 0.7012240886688232 \n",
      "Iteration 428 Epoch 0: D Loss = 1.3860135078430176, G Loss = 0.7011668086051941 \n",
      "Iteration 430 Epoch 0: D Loss = 1.3859455585479736, G Loss = 0.7010959386825562 \n",
      "Iteration 432 Epoch 0: D Loss = 1.385969877243042, G Loss = 0.7012290358543396 \n",
      "Iteration 434 Epoch 0: D Loss = 1.3859844207763672, G Loss = 0.7011279463768005 \n",
      "Iteration 436 Epoch 0: D Loss = 1.3860867023468018, G Loss = 0.7011488676071167 \n",
      "Iteration 438 Epoch 0: D Loss = 1.3860418796539307, G Loss = 0.7011411190032959 \n",
      "Iteration 440 Epoch 0: D Loss = 1.3859775066375732, G Loss = 0.7011209726333618 \n",
      "Iteration 442 Epoch 0: D Loss = 1.3859848976135254, G Loss = 0.7011138200759888 \n",
      "Iteration 444 Epoch 0: D Loss = 1.3858978748321533, G Loss = 0.7011270523071289 \n",
      "Iteration 446 Epoch 0: D Loss = 1.3860719203948975, G Loss = 0.7012020349502563 \n",
      "Iteration 448 Epoch 0: D Loss = 1.3859953880310059, G Loss = 0.701080322265625 \n",
      "Iteration 450 Epoch 0: D Loss = 1.3859138488769531, G Loss = 0.7010232210159302 \n",
      "Iteration 452 Epoch 0: D Loss = 1.385988473892212, G Loss = 0.7011268138885498 \n",
      "Iteration 454 Epoch 0: D Loss = 1.3859224319458008, G Loss = 0.7011138200759888 \n",
      "Iteration 456 Epoch 0: D Loss = 1.3859403133392334, G Loss = 0.7011004090309143 \n",
      "Iteration 458 Epoch 0: D Loss = 1.3859083652496338, G Loss = 0.7011050581932068 \n",
      "Iteration 460 Epoch 0: D Loss = 1.3860437870025635, G Loss = 0.701000988483429 \n",
      "Iteration 462 Epoch 0: D Loss = 1.3860015869140625, G Loss = 0.7011467814445496 \n",
      "Iteration 464 Epoch 0: D Loss = 1.386047124862671, G Loss = 0.701066792011261 \n",
      "Iteration 466 Epoch 0: D Loss = 1.3859403133392334, G Loss = 0.7011216282844543 \n",
      "Iteration 468 Epoch 0: D Loss = 1.3860664367675781, G Loss = 0.7012529373168945 \n",
      "Iteration 470 Epoch 0: D Loss = 1.3859291076660156, G Loss = 0.7011374235153198 \n",
      "Iteration 472 Epoch 0: D Loss = 1.3859131336212158, G Loss = 0.7010844945907593 \n",
      "Iteration 474 Epoch 0: D Loss = 1.386000633239746, G Loss = 0.7011918425559998 \n",
      "Iteration 476 Epoch 0: D Loss = 1.3859519958496094, G Loss = 0.7010350227355957 \n",
      "Iteration 478 Epoch 0: D Loss = 1.3859317302703857, G Loss = 0.7010627388954163 \n",
      "Iteration 480 Epoch 0: D Loss = 1.3859729766845703, G Loss = 0.7009511590003967 \n",
      "Iteration 482 Epoch 0: D Loss = 1.3859939575195312, G Loss = 0.7010711431503296 \n",
      "Iteration 484 Epoch 0: D Loss = 1.385969877243042, G Loss = 0.7011188268661499 \n",
      "Iteration 486 Epoch 0: D Loss = 1.3858978748321533, G Loss = 0.7011458873748779 \n",
      "Iteration 488 Epoch 0: D Loss = 1.3859152793884277, G Loss = 0.701168954372406 \n",
      "Iteration 490 Epoch 0: D Loss = 1.3859739303588867, G Loss = 0.7010363936424255 \n",
      "Iteration 492 Epoch 0: D Loss = 1.3858726024627686, G Loss = 0.7011246085166931 \n",
      "Iteration 494 Epoch 0: D Loss = 1.3859689235687256, G Loss = 0.7011483907699585 \n",
      "Iteration 496 Epoch 0: D Loss = 1.3859747648239136, G Loss = 0.70103520154953 \n",
      "Iteration 498 Epoch 0: D Loss = 1.3858816623687744, G Loss = 0.701087236404419 \n",
      "Iteration 500 Epoch 0: D Loss = 1.385854959487915, G Loss = 0.7010908722877502 \n",
      "Iteration 502 Epoch 0: D Loss = 1.3859889507293701, G Loss = 0.7010440826416016 \n",
      "Iteration 504 Epoch 0: D Loss = 1.385907769203186, G Loss = 0.7010815143585205 \n",
      "Iteration 506 Epoch 0: D Loss = 1.3859331607818604, G Loss = 0.7011674046516418 \n",
      "Iteration 508 Epoch 0: D Loss = 1.3859440088272095, G Loss = 0.7010682821273804 \n",
      "Iteration 510 Epoch 0: D Loss = 1.385969877243042, G Loss = 0.701021671295166 \n",
      "Iteration 512 Epoch 0: D Loss = 1.3859002590179443, G Loss = 0.7012156844139099 \n",
      "Iteration 514 Epoch 0: D Loss = 1.3858897686004639, G Loss = 0.7011175155639648 \n",
      "Iteration 516 Epoch 0: D Loss = 1.3860418796539307, G Loss = 0.7009667754173279 \n",
      "Iteration 518 Epoch 0: D Loss = 1.3860046863555908, G Loss = 0.7010278105735779 \n",
      "Iteration 520 Epoch 0: D Loss = 1.385976791381836, G Loss = 0.7011283040046692 \n",
      "Iteration 522 Epoch 0: D Loss = 1.385953426361084, G Loss = 0.7011034488677979 \n",
      "Iteration 524 Epoch 0: D Loss = 1.3858832120895386, G Loss = 0.7010090351104736 \n",
      "Iteration 526 Epoch 0: D Loss = 1.385884165763855, G Loss = 0.7010425925254822 \n",
      "Iteration 528 Epoch 0: D Loss = 1.3859211206436157, G Loss = 0.7009387612342834 \n",
      "Iteration 530 Epoch 0: D Loss = 1.3859050273895264, G Loss = 0.7011600732803345 \n",
      "Iteration 532 Epoch 0: D Loss = 1.3859577178955078, G Loss = 0.7011493444442749 \n",
      "Iteration 534 Epoch 0: D Loss = 1.3858308792114258, G Loss = 0.7011175751686096 \n",
      "Iteration 536 Epoch 0: D Loss = 1.3858656883239746, G Loss = 0.7010741829872131 \n",
      "Iteration 538 Epoch 0: D Loss = 1.3858556747436523, G Loss = 0.7010359168052673 \n",
      "Iteration 540 Epoch 0: D Loss = 1.3858578205108643, G Loss = 0.7010648250579834 \n",
      "Iteration 542 Epoch 0: D Loss = 1.3857930898666382, G Loss = 0.7010846734046936 \n",
      "Iteration 544 Epoch 0: D Loss = 1.3858370780944824, G Loss = 0.7011096477508545 \n",
      "Iteration 546 Epoch 0: D Loss = 1.385934591293335, G Loss = 0.7010537385940552 \n",
      "Iteration 548 Epoch 0: D Loss = 1.3858330249786377, G Loss = 0.7010202407836914 \n",
      "Iteration 550 Epoch 0: D Loss = 1.3856862783432007, G Loss = 0.7011097073554993 \n",
      "Iteration 552 Epoch 0: D Loss = 1.3858405351638794, G Loss = 0.701062798500061 \n",
      "Iteration 554 Epoch 0: D Loss = 1.3858997821807861, G Loss = 0.701111376285553 \n",
      "Iteration 556 Epoch 0: D Loss = 1.3859758377075195, G Loss = 0.7009932398796082 \n",
      "Iteration 558 Epoch 0: D Loss = 1.3859115839004517, G Loss = 0.7010243535041809 \n",
      "Iteration 560 Epoch 0: D Loss = 1.3859137296676636, G Loss = 0.7010510563850403 \n",
      "Iteration 562 Epoch 0: D Loss = 1.385788917541504, G Loss = 0.7011284232139587 \n",
      "Iteration 564 Epoch 0: D Loss = 1.3858956098556519, G Loss = 0.7010587453842163 \n",
      "Iteration 566 Epoch 0: D Loss = 1.3858119249343872, G Loss = 0.70098876953125 \n",
      "Iteration 568 Epoch 0: D Loss = 1.3859188556671143, G Loss = 0.7010778188705444 \n",
      "Iteration 570 Epoch 0: D Loss = 1.3858859539031982, G Loss = 0.7010458707809448 \n",
      "Iteration 572 Epoch 0: D Loss = 1.3859065771102905, G Loss = 0.7010850310325623 \n",
      "Iteration 574 Epoch 0: D Loss = 1.3859899044036865, G Loss = 0.7009878158569336 \n",
      "Iteration 576 Epoch 0: D Loss = 1.3858579397201538, G Loss = 0.7011072635650635 \n",
      "Iteration 578 Epoch 0: D Loss = 1.3858532905578613, G Loss = 0.7010769248008728 \n",
      "Iteration 580 Epoch 0: D Loss = 1.3858418464660645, G Loss = 0.7010090947151184 \n",
      "Iteration 582 Epoch 0: D Loss = 1.3858323097229004, G Loss = 0.7010290026664734 \n",
      "Iteration 584 Epoch 0: D Loss = 1.3858718872070312, G Loss = 0.7010104656219482 \n",
      "Iteration 586 Epoch 0: D Loss = 1.3858046531677246, G Loss = 0.7010934352874756 \n",
      "Iteration 588 Epoch 0: D Loss = 1.3858933448791504, G Loss = 0.7010322213172913 \n",
      "Iteration 590 Epoch 0: D Loss = 1.3859139680862427, G Loss = 0.7010074853897095 \n",
      "Iteration 592 Epoch 0: D Loss = 1.3858373165130615, G Loss = 0.70100337266922 \n",
      "Iteration 594 Epoch 0: D Loss = 1.3858650922775269, G Loss = 0.7009556889533997 \n",
      "Iteration 596 Epoch 0: D Loss = 1.385964035987854, G Loss = 0.7010719180107117 \n",
      "Iteration 598 Epoch 0: D Loss = 1.3857941627502441, G Loss = 0.7009094953536987 \n",
      "Iteration 600 Epoch 0: D Loss = 1.385777235031128, G Loss = 0.7010969519615173 \n",
      "Iteration 602 Epoch 0: D Loss = 1.3857572078704834, G Loss = 0.7008594274520874 \n",
      "Iteration 604 Epoch 0: D Loss = 1.3858325481414795, G Loss = 0.7009719610214233 \n",
      "Iteration 606 Epoch 0: D Loss = 1.3858370780944824, G Loss = 0.7010203003883362 \n",
      "Iteration 608 Epoch 0: D Loss = 1.385906457901001, G Loss = 0.7009769678115845 \n",
      "Iteration 610 Epoch 0: D Loss = 1.3858165740966797, G Loss = 0.7010733485221863 \n",
      "Iteration 612 Epoch 0: D Loss = 1.3858535289764404, G Loss = 0.7010073065757751 \n",
      "Iteration 614 Epoch 0: D Loss = 1.3858174085617065, G Loss = 0.701038122177124 \n",
      "Iteration 616 Epoch 0: D Loss = 1.3858528137207031, G Loss = 0.7009680867195129 \n",
      "Iteration 618 Epoch 0: D Loss = 1.385749340057373, G Loss = 0.701067328453064 \n",
      "Iteration 620 Epoch 0: D Loss = 1.3858225345611572, G Loss = 0.7009567618370056 \n",
      "Iteration 622 Epoch 0: D Loss = 1.3857831954956055, G Loss = 0.7010538578033447 \n",
      "Iteration 624 Epoch 0: D Loss = 1.3857815265655518, G Loss = 0.7009538412094116 \n",
      "Iteration 626 Epoch 0: D Loss = 1.3857676982879639, G Loss = 0.70101398229599 \n",
      "Iteration 628 Epoch 0: D Loss = 1.3858274221420288, G Loss = 0.7010443806648254 \n",
      "Iteration 630 Epoch 0: D Loss = 1.3859117031097412, G Loss = 0.7009771466255188 \n",
      "Iteration 632 Epoch 0: D Loss = 1.385665774345398, G Loss = 0.7010455131530762 \n",
      "Iteration 634 Epoch 0: D Loss = 1.385655164718628, G Loss = 0.7011041045188904 \n",
      "Iteration 636 Epoch 0: D Loss = 1.3857183456420898, G Loss = 0.7010584473609924 \n",
      "Iteration 638 Epoch 0: D Loss = 1.3858730792999268, G Loss = 0.7009432315826416 \n",
      "Iteration 640 Epoch 0: D Loss = 1.3856959342956543, G Loss = 0.7010279297828674 \n",
      "Iteration 642 Epoch 0: D Loss = 1.3857102394104004, G Loss = 0.7009158730506897 \n",
      "Iteration 644 Epoch 0: D Loss = 1.3858020305633545, G Loss = 0.7010400295257568 \n",
      "Iteration 646 Epoch 0: D Loss = 1.3858243227005005, G Loss = 0.7010349035263062 \n",
      "Iteration 648 Epoch 0: D Loss = 1.38591468334198, G Loss = 0.7010030746459961 \n",
      "Iteration 650 Epoch 0: D Loss = 1.385754108428955, G Loss = 0.7009621858596802 \n",
      "Iteration 652 Epoch 0: D Loss = 1.3856546878814697, G Loss = 0.70103919506073 \n",
      "Iteration 654 Epoch 0: D Loss = 1.3856492042541504, G Loss = 0.7009261250495911 \n",
      "Iteration 656 Epoch 0: D Loss = 1.385709285736084, G Loss = 0.7009276747703552 \n",
      "Iteration 658 Epoch 0: D Loss = 1.3858039379119873, G Loss = 0.7009201049804688 \n",
      "Iteration 660 Epoch 0: D Loss = 1.3857364654541016, G Loss = 0.7008541822433472 \n",
      "Iteration 662 Epoch 0: D Loss = 1.3856728076934814, G Loss = 0.7010365128517151 \n",
      "Iteration 664 Epoch 0: D Loss = 1.385688304901123, G Loss = 0.7008730173110962 \n",
      "Iteration 666 Epoch 0: D Loss = 1.385777473449707, G Loss = 0.7008999586105347 \n",
      "Iteration 668 Epoch 0: D Loss = 1.3857989311218262, G Loss = 0.7009603977203369 \n",
      "Iteration 670 Epoch 0: D Loss = 1.3857474327087402, G Loss = 0.7008894085884094 \n",
      "Iteration 672 Epoch 0: D Loss = 1.3856909275054932, G Loss = 0.7009310126304626 \n",
      "Iteration 674 Epoch 0: D Loss = 1.3858211040496826, G Loss = 0.7010387778282166 \n",
      "Iteration 676 Epoch 0: D Loss = 1.3856985569000244, G Loss = 0.7009174823760986 \n",
      "Iteration 678 Epoch 0: D Loss = 1.3857922554016113, G Loss = 0.7009415626525879 \n",
      "Iteration 680 Epoch 0: D Loss = 1.3857479095458984, G Loss = 0.700933575630188 \n",
      "Iteration 682 Epoch 0: D Loss = 1.3857226371765137, G Loss = 0.7009742856025696 \n",
      "Iteration 684 Epoch 0: D Loss = 1.3857226371765137, G Loss = 0.7010132074356079 \n",
      "Iteration 686 Epoch 0: D Loss = 1.385763168334961, G Loss = 0.7009609937667847 \n",
      "Iteration 688 Epoch 0: D Loss = 1.3857396841049194, G Loss = 0.701032280921936 \n",
      "Iteration 690 Epoch 0: D Loss = 1.3857948780059814, G Loss = 0.7009660005569458 \n",
      "Iteration 692 Epoch 0: D Loss = 1.3856444358825684, G Loss = 0.7008985280990601 \n",
      "Iteration 694 Epoch 0: D Loss = 1.3857570886611938, G Loss = 0.7009446024894714 \n",
      "Iteration 696 Epoch 0: D Loss = 1.3855950832366943, G Loss = 0.7009373307228088 \n",
      "Iteration 698 Epoch 0: D Loss = 1.3857359886169434, G Loss = 0.7009314298629761 \n",
      "Iteration 700 Epoch 0: D Loss = 1.385664463043213, G Loss = 0.7008519172668457 \n",
      "Iteration 702 Epoch 0: D Loss = 1.3857648372650146, G Loss = 0.7009644508361816 \n",
      "Iteration 704 Epoch 0: D Loss = 1.3857743740081787, G Loss = 0.700980007648468 \n",
      "Iteration 706 Epoch 0: D Loss = 1.3856096267700195, G Loss = 0.7007553577423096 \n",
      "Iteration 708 Epoch 0: D Loss = 1.3857274055480957, G Loss = 0.7009422183036804 \n",
      "Iteration 710 Epoch 0: D Loss = 1.3856837749481201, G Loss = 0.7008217573165894 \n",
      "Iteration 712 Epoch 0: D Loss = 1.3857309818267822, G Loss = 0.7008824348449707 \n",
      "Iteration 714 Epoch 0: D Loss = 1.3857406377792358, G Loss = 0.7009548544883728 \n",
      "Iteration 716 Epoch 0: D Loss = 1.3857667446136475, G Loss = 0.7008519172668457 \n",
      "Iteration 718 Epoch 0: D Loss = 1.385699987411499, G Loss = 0.7008874416351318 \n",
      "Iteration 720 Epoch 0: D Loss = 1.3857488632202148, G Loss = 0.7008970379829407 \n",
      "Iteration 722 Epoch 0: D Loss = 1.3856996297836304, G Loss = 0.7009755969047546 \n",
      "Iteration 724 Epoch 0: D Loss = 1.385629415512085, G Loss = 0.7008721828460693 \n",
      "Iteration 726 Epoch 0: D Loss = 1.3857455253601074, G Loss = 0.701022744178772 \n",
      "Iteration 728 Epoch 0: D Loss = 1.38572096824646, G Loss = 0.7007869482040405 \n",
      "Iteration 730 Epoch 0: D Loss = 1.3857662677764893, G Loss = 0.7009104490280151 \n",
      "Iteration 732 Epoch 0: D Loss = 1.3856027126312256, G Loss = 0.7008700370788574 \n",
      "Iteration 734 Epoch 0: D Loss = 1.3856587409973145, G Loss = 0.7007564306259155 \n",
      "Iteration 736 Epoch 0: D Loss = 1.385584831237793, G Loss = 0.7008808255195618 \n",
      "Iteration 738 Epoch 0: D Loss = 1.3857368230819702, G Loss = 0.7009801268577576 \n",
      "Iteration 740 Epoch 0: D Loss = 1.3857513666152954, G Loss = 0.7009718418121338 \n",
      "Iteration 742 Epoch 0: D Loss = 1.3856587409973145, G Loss = 0.7008301019668579 \n",
      "Iteration 744 Epoch 0: D Loss = 1.3856370449066162, G Loss = 0.7009177803993225 \n",
      "Iteration 746 Epoch 0: D Loss = 1.3857057094573975, G Loss = 0.7009276151657104 \n",
      "Iteration 748 Epoch 0: D Loss = 1.3855680227279663, G Loss = 0.7009462118148804 \n",
      "Iteration 750 Epoch 0: D Loss = 1.385488748550415, G Loss = 0.7008890509605408 \n",
      "Iteration 752 Epoch 0: D Loss = 1.3855992555618286, G Loss = 0.7008731365203857 \n",
      "Iteration 754 Epoch 0: D Loss = 1.3855355978012085, G Loss = 0.7009061574935913 \n",
      "Iteration 756 Epoch 0: D Loss = 1.3855725526809692, G Loss = 0.7007955312728882 \n",
      "Iteration 758 Epoch 0: D Loss = 1.3856432437896729, G Loss = 0.7008448839187622 \n",
      "Iteration 760 Epoch 0: D Loss = 1.385664701461792, G Loss = 0.7008543610572815 \n",
      "Iteration 762 Epoch 0: D Loss = 1.3857605457305908, G Loss = 0.700855016708374 \n",
      "Iteration 764 Epoch 0: D Loss = 1.385546326637268, G Loss = 0.7008671760559082 \n",
      "Iteration 766 Epoch 0: D Loss = 1.3856180906295776, G Loss = 0.7008983492851257 \n",
      "Iteration 768 Epoch 0: D Loss = 1.3856309652328491, G Loss = 0.7009092569351196 \n",
      "Iteration 770 Epoch 0: D Loss = 1.3857148885726929, G Loss = 0.7008762359619141 \n",
      "Iteration 772 Epoch 0: D Loss = 1.385518193244934, G Loss = 0.700800895690918 \n",
      "Iteration 774 Epoch 0: D Loss = 1.385587453842163, G Loss = 0.7008110880851746 \n",
      "Iteration 776 Epoch 0: D Loss = 1.3856284618377686, G Loss = 0.7007490992546082 \n",
      "Iteration 778 Epoch 0: D Loss = 1.3856420516967773, G Loss = 0.7009109258651733 \n",
      "Iteration 780 Epoch 0: D Loss = 1.3856104612350464, G Loss = 0.7008476853370667 \n",
      "Iteration 782 Epoch 0: D Loss = 1.3856050968170166, G Loss = 0.7008638381958008 \n",
      "Iteration 784 Epoch 0: D Loss = 1.385622262954712, G Loss = 0.7008293867111206 \n",
      "Iteration 786 Epoch 0: D Loss = 1.3856308460235596, G Loss = 0.7008357048034668 \n",
      "Iteration 788 Epoch 0: D Loss = 1.3855934143066406, G Loss = 0.7008079886436462 \n",
      "Iteration 790 Epoch 0: D Loss = 1.3855854272842407, G Loss = 0.700866162776947 \n",
      "Iteration 792 Epoch 0: D Loss = 1.3855000734329224, G Loss = 0.7008742094039917 \n",
      "Iteration 794 Epoch 0: D Loss = 1.3856513500213623, G Loss = 0.70085608959198 \n",
      "Iteration 796 Epoch 0: D Loss = 1.3855414390563965, G Loss = 0.7008764147758484 \n",
      "Iteration 798 Epoch 0: D Loss = 1.3855903148651123, G Loss = 0.7007657885551453 \n",
      "Iteration 800 Epoch 0: D Loss = 1.3855669498443604, G Loss = 0.7008234262466431 \n",
      "Iteration 802 Epoch 0: D Loss = 1.385546088218689, G Loss = 0.7008379697799683 \n",
      "Iteration 804 Epoch 0: D Loss = 1.3855116367340088, G Loss = 0.7007727026939392 \n",
      "Iteration 806 Epoch 0: D Loss = 1.3856128454208374, G Loss = 0.7008503675460815 \n",
      "Iteration 808 Epoch 0: D Loss = 1.3855550289154053, G Loss = 0.7008880376815796 \n",
      "Iteration 810 Epoch 0: D Loss = 1.38554048538208, G Loss = 0.7008262872695923 \n",
      "Iteration 812 Epoch 0: D Loss = 1.3855918645858765, G Loss = 0.7007470726966858 \n",
      "Iteration 814 Epoch 0: D Loss = 1.3857004642486572, G Loss = 0.7007365822792053 \n",
      "Iteration 816 Epoch 0: D Loss = 1.3856079578399658, G Loss = 0.700819194316864 \n",
      "Iteration 818 Epoch 0: D Loss = 1.3855013847351074, G Loss = 0.7008684873580933 \n",
      "Iteration 820 Epoch 0: D Loss = 1.3855851888656616, G Loss = 0.7007575035095215 \n",
      "Iteration 822 Epoch 0: D Loss = 1.3855979442596436, G Loss = 0.7008466720581055 \n",
      "Iteration 824 Epoch 0: D Loss = 1.3854975700378418, G Loss = 0.7008956670761108 \n",
      "Iteration 826 Epoch 0: D Loss = 1.3856208324432373, G Loss = 0.700785756111145 \n",
      "Iteration 828 Epoch 0: D Loss = 1.3856093883514404, G Loss = 0.7008213996887207 \n",
      "Iteration 830 Epoch 0: D Loss = 1.3855631351470947, G Loss = 0.7009324431419373 \n",
      "Iteration 832 Epoch 0: D Loss = 1.3854918479919434, G Loss = 0.7007556557655334 \n",
      "Iteration 834 Epoch 0: D Loss = 1.3855868577957153, G Loss = 0.7008029818534851 \n",
      "Iteration 836 Epoch 0: D Loss = 1.385711431503296, G Loss = 0.7008101344108582 \n",
      "Iteration 838 Epoch 0: D Loss = 1.3855031728744507, G Loss = 0.7008511424064636 \n",
      "Iteration 840 Epoch 0: D Loss = 1.3856663703918457, G Loss = 0.7007931470870972 \n",
      "Iteration 842 Epoch 0: D Loss = 1.3855538368225098, G Loss = 0.700812041759491 \n",
      "Iteration 844 Epoch 0: D Loss = 1.3856569528579712, G Loss = 0.7008650302886963 \n",
      "Iteration 846 Epoch 0: D Loss = 1.3854979276657104, G Loss = 0.7008517384529114 \n",
      "Iteration 848 Epoch 0: D Loss = 1.3853614330291748, G Loss = 0.7008309364318848 \n",
      "Iteration 850 Epoch 0: D Loss = 1.3854507207870483, G Loss = 0.7008388042449951 \n",
      "Iteration 852 Epoch 0: D Loss = 1.3855806589126587, G Loss = 0.7008517384529114 \n",
      "Iteration 854 Epoch 0: D Loss = 1.3855774402618408, G Loss = 0.7007771730422974 \n",
      "Iteration 856 Epoch 0: D Loss = 1.3855661153793335, G Loss = 0.7007681727409363 \n",
      "Iteration 858 Epoch 0: D Loss = 1.3855725526809692, G Loss = 0.7009530663490295 \n",
      "Iteration 860 Epoch 0: D Loss = 1.3854880332946777, G Loss = 0.7008005976676941 \n",
      "Iteration 862 Epoch 0: D Loss = 1.3855109214782715, G Loss = 0.7008947134017944 \n",
      "Iteration 864 Epoch 0: D Loss = 1.3855137825012207, G Loss = 0.700806736946106 \n",
      "Iteration 866 Epoch 0: D Loss = 1.3855245113372803, G Loss = 0.7007695436477661 \n",
      "Iteration 868 Epoch 0: D Loss = 1.385491967201233, G Loss = 0.7008334398269653 \n",
      "Iteration 870 Epoch 0: D Loss = 1.3854652643203735, G Loss = 0.7007737159729004 \n",
      "Iteration 872 Epoch 0: D Loss = 1.3854389190673828, G Loss = 0.7008397579193115 \n",
      "Iteration 874 Epoch 0: D Loss = 1.385540246963501, G Loss = 0.700832188129425 \n",
      "Iteration 876 Epoch 0: D Loss = 1.3854596614837646, G Loss = 0.7007737159729004 \n",
      "Iteration 878 Epoch 0: D Loss = 1.3855867385864258, G Loss = 0.7008539438247681 \n",
      "Iteration 880 Epoch 0: D Loss = 1.3854584693908691, G Loss = 0.7008666396141052 \n",
      "Iteration 882 Epoch 0: D Loss = 1.3855254650115967, G Loss = 0.7007676959037781 \n",
      "Iteration 884 Epoch 0: D Loss = 1.3854784965515137, G Loss = 0.7007240056991577 \n",
      "Iteration 886 Epoch 0: D Loss = 1.3854520320892334, G Loss = 0.7008087635040283 \n",
      "Iteration 888 Epoch 0: D Loss = 1.3854340314865112, G Loss = 0.7006189227104187 \n",
      "Iteration 890 Epoch 0: D Loss = 1.3854334354400635, G Loss = 0.7007678151130676 \n",
      "Iteration 892 Epoch 0: D Loss = 1.3853856325149536, G Loss = 0.7007594704627991 \n",
      "Iteration 894 Epoch 0: D Loss = 1.3854453563690186, G Loss = 0.7008441090583801 \n",
      "Iteration 896 Epoch 0: D Loss = 1.3855140209197998, G Loss = 0.7007810473442078 \n",
      "Iteration 898 Epoch 0: D Loss = 1.385524868965149, G Loss = 0.7008216977119446 \n",
      "Iteration 900 Epoch 0: D Loss = 1.385505199432373, G Loss = 0.7007126808166504 \n",
      "Iteration 902 Epoch 0: D Loss = 1.3853604793548584, G Loss = 0.7007182240486145 \n",
      "Iteration 904 Epoch 0: D Loss = 1.3854109048843384, G Loss = 0.7007496356964111 \n",
      "Iteration 906 Epoch 0: D Loss = 1.3855781555175781, G Loss = 0.7007942199707031 \n",
      "Iteration 908 Epoch 0: D Loss = 1.3855700492858887, G Loss = 0.700680136680603 \n",
      "Iteration 910 Epoch 0: D Loss = 1.3854012489318848, G Loss = 0.7008694410324097 \n",
      "Iteration 912 Epoch 0: D Loss = 1.3854331970214844, G Loss = 0.7006444931030273 \n",
      "Iteration 914 Epoch 0: D Loss = 1.3854544162750244, G Loss = 0.7006746530532837 \n",
      "Iteration 916 Epoch 0: D Loss = 1.3855397701263428, G Loss = 0.7007285356521606 \n",
      "Iteration 918 Epoch 0: D Loss = 1.3854089975357056, G Loss = 0.7007919549942017 \n",
      "Iteration 920 Epoch 0: D Loss = 1.3854647874832153, G Loss = 0.7007279992103577 \n",
      "Iteration 922 Epoch 0: D Loss = 1.385473370552063, G Loss = 0.7006920576095581 \n",
      "Iteration 924 Epoch 0: D Loss = 1.3854990005493164, G Loss = 0.7006989121437073 \n",
      "Iteration 926 Epoch 0: D Loss = 1.3856271505355835, G Loss = 0.7008717060089111 \n",
      "Iteration 928 Epoch 0: D Loss = 1.3854191303253174, G Loss = 0.7007492184638977 \n",
      "Iteration 930 Epoch 0: D Loss = 1.3855823278427124, G Loss = 0.7007707357406616 \n",
      "Iteration 932 Epoch 0: D Loss = 1.3855030536651611, G Loss = 0.7007426023483276 \n",
      "Iteration 934 Epoch 0: D Loss = 1.3853778839111328, G Loss = 0.7008102536201477 \n",
      "Iteration 936 Epoch 0: D Loss = 1.3854954242706299, G Loss = 0.7007421851158142 \n",
      "Iteration 938 Epoch 0: D Loss = 1.3855454921722412, G Loss = 0.7007452249526978 \n",
      "Iteration 940 Epoch 0: D Loss = 1.3854397535324097, G Loss = 0.7008215188980103 \n",
      "Iteration 942 Epoch 0: D Loss = 1.385478138923645, G Loss = 0.7006815671920776 \n",
      "Iteration 944 Epoch 0: D Loss = 1.3854577541351318, G Loss = 0.7007075548171997 \n",
      "Iteration 946 Epoch 0: D Loss = 1.3854749202728271, G Loss = 0.7007757425308228 \n",
      "Iteration 948 Epoch 0: D Loss = 1.3853750228881836, G Loss = 0.7006353139877319 \n",
      "Iteration 950 Epoch 0: D Loss = 1.3853591680526733, G Loss = 0.7007214426994324 \n",
      "Iteration 952 Epoch 0: D Loss = 1.3853890895843506, G Loss = 0.700655460357666 \n",
      "Iteration 954 Epoch 0: D Loss = 1.385392427444458, G Loss = 0.7007326483726501 \n",
      "Iteration 956 Epoch 0: D Loss = 1.3853695392608643, G Loss = 0.7007572650909424 \n",
      "Iteration 958 Epoch 0: D Loss = 1.3854577541351318, G Loss = 0.7007338404655457 \n",
      "Iteration 960 Epoch 0: D Loss = 1.3853669166564941, G Loss = 0.7007191777229309 \n",
      "Iteration 962 Epoch 0: D Loss = 1.3853654861450195, G Loss = 0.7007546424865723 \n",
      "Iteration 964 Epoch 0: D Loss = 1.3853999376296997, G Loss = 0.7008026838302612 \n",
      "Iteration 966 Epoch 0: D Loss = 1.385453462600708, G Loss = 0.7006906270980835 \n",
      "Iteration 968 Epoch 0: D Loss = 1.3853352069854736, G Loss = 0.7007461786270142 \n",
      "Iteration 970 Epoch 0: D Loss = 1.385420322418213, G Loss = 0.7007645964622498 \n",
      "Iteration 972 Epoch 0: D Loss = 1.3854424953460693, G Loss = 0.7007401585578918 \n",
      "Iteration 974 Epoch 0: D Loss = 1.3853940963745117, G Loss = 0.7007131576538086 \n",
      "Iteration 976 Epoch 0: D Loss = 1.3853448629379272, G Loss = 0.7006175518035889 \n",
      "Iteration 978 Epoch 0: D Loss = 1.3853442668914795, G Loss = 0.7006561160087585 \n",
      "Iteration 980 Epoch 0: D Loss = 1.385385274887085, G Loss = 0.7006236910820007 \n",
      "Iteration 982 Epoch 0: D Loss = 1.3854142427444458, G Loss = 0.7007817029953003 \n",
      "Iteration 984 Epoch 0: D Loss = 1.385425329208374, G Loss = 0.7007402181625366 \n",
      "Iteration 986 Epoch 0: D Loss = 1.3854457139968872, G Loss = 0.7008711099624634 \n",
      "Iteration 988 Epoch 0: D Loss = 1.3854100704193115, G Loss = 0.7007917165756226 \n",
      "Iteration 990 Epoch 0: D Loss = 1.3853384256362915, G Loss = 0.7007122039794922 \n",
      "Iteration 992 Epoch 0: D Loss = 1.3854297399520874, G Loss = 0.7007355690002441 \n",
      "Iteration 994 Epoch 0: D Loss = 1.3855069875717163, G Loss = 0.7006689310073853 \n",
      "Iteration 996 Epoch 0: D Loss = 1.3855104446411133, G Loss = 0.7006458044052124 \n",
      "Iteration 998 Epoch 0: D Loss = 1.3854228258132935, G Loss = 0.7006829380989075 \n",
      "Iteration 1000 Epoch 0: D Loss = 1.3853588104248047, G Loss = 0.7006046772003174 \n",
      "Iteration 1002 Epoch 0: D Loss = 1.3853970766067505, G Loss = 0.7006972432136536 \n",
      "Iteration 1004 Epoch 0: D Loss = 1.3854048252105713, G Loss = 0.7007075548171997 \n",
      "Iteration 1006 Epoch 0: D Loss = 1.385312795639038, G Loss = 0.7007639408111572 \n",
      "Iteration 1008 Epoch 0: D Loss = 1.3853189945220947, G Loss = 0.7006511092185974 \n",
      "Iteration 1010 Epoch 0: D Loss = 1.3853769302368164, G Loss = 0.7005800604820251 \n",
      "Iteration 1012 Epoch 0: D Loss = 1.385380744934082, G Loss = 0.7007217407226562 \n",
      "Iteration 1014 Epoch 0: D Loss = 1.3854506015777588, G Loss = 0.700695812702179 \n",
      "Iteration 1016 Epoch 0: D Loss = 1.3854126930236816, G Loss = 0.7005802989006042 \n",
      "Iteration 1018 Epoch 0: D Loss = 1.3854420185089111, G Loss = 0.7006510496139526 \n",
      "Iteration 1020 Epoch 0: D Loss = 1.3854150772094727, G Loss = 0.7006617188453674 \n",
      "Iteration 1022 Epoch 0: D Loss = 1.385312795639038, G Loss = 0.7007209658622742 \n",
      "Iteration 1024 Epoch 0: D Loss = 1.3854541778564453, G Loss = 0.7006593346595764 \n",
      "Iteration 1026 Epoch 0: D Loss = 1.385367751121521, G Loss = 0.7006450295448303 \n",
      "Iteration 1028 Epoch 0: D Loss = 1.3853051662445068, G Loss = 0.7006190419197083 \n",
      "Iteration 1030 Epoch 0: D Loss = 1.3854035139083862, G Loss = 0.7005845308303833 \n",
      "Iteration 1032 Epoch 0: D Loss = 1.3853332996368408, G Loss = 0.7007024884223938 \n",
      "Iteration 1034 Epoch 0: D Loss = 1.3852972984313965, G Loss = 0.7007939219474792 \n",
      "Iteration 1036 Epoch 0: D Loss = 1.3852664232254028, G Loss = 0.700600802898407 \n",
      "Iteration 1038 Epoch 0: D Loss = 1.3854551315307617, G Loss = 0.7005981802940369 \n",
      "Iteration 1040 Epoch 0: D Loss = 1.3852895498275757, G Loss = 0.7006381750106812 \n",
      "Iteration 1042 Epoch 0: D Loss = 1.3853017091751099, G Loss = 0.7005706429481506 \n",
      "Iteration 1044 Epoch 0: D Loss = 1.385270118713379, G Loss = 0.7007043361663818 \n",
      "Iteration 1046 Epoch 0: D Loss = 1.3853042125701904, G Loss = 0.7006115317344666 \n",
      "Iteration 1048 Epoch 0: D Loss = 1.3852818012237549, G Loss = 0.7006847858428955 \n",
      "Iteration 1050 Epoch 0: D Loss = 1.3853517770767212, G Loss = 0.700589120388031 \n",
      "Iteration 1052 Epoch 0: D Loss = 1.3854776620864868, G Loss = 0.700613260269165 \n",
      "Iteration 1054 Epoch 0: D Loss = 1.3854087591171265, G Loss = 0.7007204294204712 \n",
      "Iteration 1056 Epoch 0: D Loss = 1.385286808013916, G Loss = 0.700639009475708 \n",
      "Iteration 1058 Epoch 0: D Loss = 1.3852529525756836, G Loss = 0.700577437877655 \n",
      "Iteration 1060 Epoch 0: D Loss = 1.3853528499603271, G Loss = 0.700580358505249 \n",
      "Iteration 1062 Epoch 0: D Loss = 1.3852564096450806, G Loss = 0.7005614042282104 \n",
      "Iteration 1064 Epoch 0: D Loss = 1.385118007659912, G Loss = 0.7006323933601379 \n",
      "Iteration 1066 Epoch 0: D Loss = 1.3853496313095093, G Loss = 0.7005726099014282 \n",
      "Iteration 1068 Epoch 0: D Loss = 1.3852698802947998, G Loss = 0.7005707025527954 \n",
      "Iteration 1070 Epoch 0: D Loss = 1.3853425979614258, G Loss = 0.7007195353507996 \n",
      "Iteration 1072 Epoch 0: D Loss = 1.38529634475708, G Loss = 0.7005996108055115 \n",
      "Iteration 1074 Epoch 0: D Loss = 1.3853142261505127, G Loss = 0.7005469799041748 \n",
      "Iteration 1076 Epoch 0: D Loss = 1.3852505683898926, G Loss = 0.7006561756134033 \n",
      "Iteration 1078 Epoch 0: D Loss = 1.3854141235351562, G Loss = 0.7006566524505615 \n",
      "Iteration 1080 Epoch 0: D Loss = 1.3854540586471558, G Loss = 0.7007383704185486 \n",
      "Iteration 1082 Epoch 0: D Loss = 1.3852548599243164, G Loss = 0.7006504535675049 \n",
      "Iteration 1084 Epoch 0: D Loss = 1.3853144645690918, G Loss = 0.7006465792655945 \n",
      "Iteration 1086 Epoch 0: D Loss = 1.38535737991333, G Loss = 0.7006205916404724 \n",
      "Iteration 1088 Epoch 0: D Loss = 1.3852694034576416, G Loss = 0.7006279826164246 \n",
      "Iteration 1090 Epoch 0: D Loss = 1.385353446006775, G Loss = 0.7005899548530579 \n",
      "Iteration 1092 Epoch 0: D Loss = 1.3851807117462158, G Loss = 0.7006611227989197 \n",
      "Iteration 1094 Epoch 0: D Loss = 1.3852059841156006, G Loss = 0.7006924152374268 \n",
      "Iteration 1096 Epoch 0: D Loss = 1.3854132890701294, G Loss = 0.7006199955940247 \n",
      "Iteration 1098 Epoch 0: D Loss = 1.3853909969329834, G Loss = 0.7006176710128784 \n",
      "Iteration 1100 Epoch 0: D Loss = 1.3853538036346436, G Loss = 0.7005380392074585 \n",
      "Iteration 1102 Epoch 0: D Loss = 1.385251522064209, G Loss = 0.700486958026886 \n",
      "Iteration 1104 Epoch 0: D Loss = 1.3852710723876953, G Loss = 0.7005536556243896 \n",
      "Iteration 1106 Epoch 0: D Loss = 1.3851896524429321, G Loss = 0.7006130218505859 \n",
      "Iteration 1108 Epoch 0: D Loss = 1.3851747512817383, G Loss = 0.7005574703216553 \n",
      "Iteration 1110 Epoch 0: D Loss = 1.385237693786621, G Loss = 0.7006010413169861 \n",
      "Iteration 1112 Epoch 0: D Loss = 1.3852289915084839, G Loss = 0.7005704045295715 \n",
      "Iteration 1114 Epoch 0: D Loss = 1.3851796388626099, G Loss = 0.7005400657653809 \n",
      "Iteration 1116 Epoch 0: D Loss = 1.3852157592773438, G Loss = 0.7006977200508118 \n",
      "Iteration 1118 Epoch 0: D Loss = 1.3851784467697144, G Loss = 0.7005911469459534 \n",
      "Iteration 1120 Epoch 0: D Loss = 1.3853572607040405, G Loss = 0.700632631778717 \n",
      "Iteration 1122 Epoch 0: D Loss = 1.3853106498718262, G Loss = 0.7006529569625854 \n",
      "Iteration 1124 Epoch 0: D Loss = 1.3851591348648071, G Loss = 0.7005351781845093 \n",
      "Iteration 1126 Epoch 0: D Loss = 1.3851923942565918, G Loss = 0.7006046175956726 \n",
      "Iteration 1128 Epoch 0: D Loss = 1.385291576385498, G Loss = 0.7006720304489136 \n",
      "Iteration 1130 Epoch 0: D Loss = 1.3851966857910156, G Loss = 0.7005481719970703 \n",
      "Iteration 1132 Epoch 0: D Loss = 1.385155200958252, G Loss = 0.7005001306533813 \n",
      "Iteration 1134 Epoch 0: D Loss = 1.3851549625396729, G Loss = 0.7005572319030762 \n",
      "Iteration 1136 Epoch 0: D Loss = 1.3852869272232056, G Loss = 0.7006270289421082 \n",
      "Iteration 1138 Epoch 0: D Loss = 1.3851892948150635, G Loss = 0.7006054520606995 \n",
      "Iteration 1140 Epoch 0: D Loss = 1.3851617574691772, G Loss = 0.7006842494010925 \n",
      "Iteration 1142 Epoch 0: D Loss = 1.3851261138916016, G Loss = 0.7005537152290344 \n",
      "Iteration 1144 Epoch 0: D Loss = 1.3852553367614746, G Loss = 0.700668215751648 \n",
      "Iteration 1146 Epoch 0: D Loss = 1.3852589130401611, G Loss = 0.7006077766418457 \n",
      "Iteration 1148 Epoch 0: D Loss = 1.385256052017212, G Loss = 0.7005649209022522 \n",
      "Iteration 1150 Epoch 0: D Loss = 1.3852834701538086, G Loss = 0.7005630135536194 \n",
      "Iteration 1152 Epoch 0: D Loss = 1.3851971626281738, G Loss = 0.7005369663238525 \n",
      "Iteration 1154 Epoch 0: D Loss = 1.3852514028549194, G Loss = 0.7005572319030762 \n",
      "Iteration 1156 Epoch 0: D Loss = 1.3852488994598389, G Loss = 0.7005568742752075 \n",
      "Iteration 1158 Epoch 0: D Loss = 1.3852365016937256, G Loss = 0.7006021738052368 \n",
      "Iteration 1160 Epoch 0: D Loss = 1.3852992057800293, G Loss = 0.7004771828651428 \n",
      "Iteration 1162 Epoch 0: D Loss = 1.3850626945495605, G Loss = 0.7005378603935242 \n",
      "Iteration 1164 Epoch 0: D Loss = 1.3852660655975342, G Loss = 0.7006001472473145 \n",
      "Iteration 1166 Epoch 0: D Loss = 1.3852297067642212, G Loss = 0.7005875110626221 \n",
      "Iteration 1168 Epoch 0: D Loss = 1.385249137878418, G Loss = 0.7005681991577148 \n",
      "Iteration 1170 Epoch 0: D Loss = 1.3851850032806396, G Loss = 0.7005020380020142 \n",
      "Iteration 1172 Epoch 0: D Loss = 1.385195255279541, G Loss = 0.7005135416984558 \n",
      "Iteration 1174 Epoch 0: D Loss = 1.385241985321045, G Loss = 0.7005600929260254 \n",
      "Iteration 1176 Epoch 0: D Loss = 1.3851370811462402, G Loss = 0.700579822063446 \n",
      "Iteration 1178 Epoch 0: D Loss = 1.385148525238037, G Loss = 0.7006422877311707 \n",
      "Iteration 1180 Epoch 0: D Loss = 1.3851051330566406, G Loss = 0.7005689740180969 \n",
      "Iteration 1182 Epoch 0: D Loss = 1.3850817680358887, G Loss = 0.7005522847175598 \n",
      "Iteration 1184 Epoch 0: D Loss = 1.3851492404937744, G Loss = 0.7005759477615356 \n",
      "Iteration 1186 Epoch 0: D Loss = 1.3851796388626099, G Loss = 0.7005207538604736 \n",
      "Iteration 1188 Epoch 0: D Loss = 1.3851783275604248, G Loss = 0.7005539536476135 \n",
      "Iteration 1190 Epoch 0: D Loss = 1.385176420211792, G Loss = 0.700563907623291 \n",
      "Iteration 1192 Epoch 0: D Loss = 1.385141372680664, G Loss = 0.7004278898239136 \n",
      "Iteration 1194 Epoch 0: D Loss = 1.385188102722168, G Loss = 0.7006407976150513 \n",
      "Iteration 1196 Epoch 0: D Loss = 1.3851580619812012, G Loss = 0.7005490660667419 \n",
      "Iteration 1198 Epoch 0: D Loss = 1.3851063251495361, G Loss = 0.7005223035812378 \n",
      "Iteration 1200 Epoch 0: D Loss = 1.3850822448730469, G Loss = 0.7004684209823608 \n",
      "Iteration 1202 Epoch 0: D Loss = 1.3852121829986572, G Loss = 0.7004484534263611 \n",
      "Iteration 1204 Epoch 0: D Loss = 1.3852790594100952, G Loss = 0.7004286050796509 \n",
      "Iteration 1206 Epoch 0: D Loss = 1.3852252960205078, G Loss = 0.7005349397659302 \n",
      "Iteration 1208 Epoch 0: D Loss = 1.3851368427276611, G Loss = 0.7005572319030762 \n",
      "Iteration 1210 Epoch 0: D Loss = 1.3852503299713135, G Loss = 0.700527012348175 \n",
      "Iteration 1212 Epoch 0: D Loss = 1.3851227760314941, G Loss = 0.7004997730255127 \n",
      "Iteration 1214 Epoch 0: D Loss = 1.3851103782653809, G Loss = 0.7005345225334167 \n",
      "Iteration 1216 Epoch 0: D Loss = 1.385143518447876, G Loss = 0.7004514932632446 \n",
      "Iteration 1218 Epoch 0: D Loss = 1.3851416110992432, G Loss = 0.700532853603363 \n",
      "Iteration 1220 Epoch 0: D Loss = 1.38521146774292, G Loss = 0.7005394697189331 \n",
      "Iteration 1222 Epoch 0: D Loss = 1.3851549625396729, G Loss = 0.7006217241287231 \n",
      "Iteration 1224 Epoch 0: D Loss = 1.385108470916748, G Loss = 0.7004178762435913 \n",
      "Iteration 1226 Epoch 0: D Loss = 1.385101079940796, G Loss = 0.7005738019943237 \n",
      "Iteration 1228 Epoch 0: D Loss = 1.385176181793213, G Loss = 0.7006193995475769 \n",
      "Iteration 1230 Epoch 0: D Loss = 1.385147213935852, G Loss = 0.7004972696304321 \n",
      "Iteration 1232 Epoch 0: D Loss = 1.3851335048675537, G Loss = 0.7004960179328918 \n",
      "Iteration 1234 Epoch 0: D Loss = 1.3851608037948608, G Loss = 0.7004250288009644 \n",
      "Iteration 1236 Epoch 0: D Loss = 1.3850197792053223, G Loss = 0.700394868850708 \n",
      "Iteration 1238 Epoch 0: D Loss = 1.385165810585022, G Loss = 0.7004045248031616 \n",
      "Iteration 1240 Epoch 0: D Loss = 1.3851163387298584, G Loss = 0.7004678249359131 \n",
      "Iteration 1242 Epoch 0: D Loss = 1.3850595951080322, G Loss = 0.7004378437995911 \n",
      "Iteration 1244 Epoch 0: D Loss = 1.3851149082183838, G Loss = 0.700504720211029 \n",
      "Iteration 1246 Epoch 0: D Loss = 1.3852317333221436, G Loss = 0.7005677819252014 \n",
      "Iteration 1248 Epoch 0: D Loss = 1.3851666450500488, G Loss = 0.7005028128623962 \n",
      "Iteration 1250 Epoch 0: D Loss = 1.3850808143615723, G Loss = 0.700471043586731 \n",
      "Iteration 1252 Epoch 0: D Loss = 1.385189175605774, G Loss = 0.7004141211509705 \n",
      "Iteration 1254 Epoch 0: D Loss = 1.3849928379058838, G Loss = 0.7004444599151611 \n",
      "Iteration 1256 Epoch 0: D Loss = 1.3851652145385742, G Loss = 0.7005195617675781 \n",
      "Iteration 1258 Epoch 0: D Loss = 1.385033369064331, G Loss = 0.7003974318504333 \n",
      "Iteration 1260 Epoch 0: D Loss = 1.3851752281188965, G Loss = 0.7004948258399963 \n",
      "Iteration 1262 Epoch 0: D Loss = 1.3851778507232666, G Loss = 0.7004881501197815 \n",
      "Iteration 1264 Epoch 0: D Loss = 1.3851935863494873, G Loss = 0.7004911303520203 \n",
      "Iteration 1266 Epoch 0: D Loss = 1.3851161003112793, G Loss = 0.7005552053451538 \n",
      "Iteration 1268 Epoch 0: D Loss = 1.3851346969604492, G Loss = 0.7004284858703613 \n",
      "Iteration 1270 Epoch 0: D Loss = 1.384934902191162, G Loss = 0.7004650235176086 \n",
      "Iteration 1272 Epoch 0: D Loss = 1.3851678371429443, G Loss = 0.7004188299179077 \n",
      "Iteration 1274 Epoch 0: D Loss = 1.3851683139801025, G Loss = 0.7004714012145996 \n",
      "Iteration 1276 Epoch 0: D Loss = 1.3850384950637817, G Loss = 0.7004459500312805 \n",
      "Iteration 1278 Epoch 0: D Loss = 1.3850390911102295, G Loss = 0.7004939317703247 \n",
      "Iteration 1280 Epoch 0: D Loss = 1.3850536346435547, G Loss = 0.7005172967910767 \n",
      "Iteration 1282 Epoch 0: D Loss = 1.3850810527801514, G Loss = 0.7005192637443542 \n",
      "Iteration 1284 Epoch 0: D Loss = 1.3850793838500977, G Loss = 0.7004262208938599 \n",
      "Iteration 1286 Epoch 0: D Loss = 1.3850338459014893, G Loss = 0.7005021572113037 \n",
      "Iteration 1288 Epoch 0: D Loss = 1.3852083683013916, G Loss = 0.700423538684845 \n",
      "Iteration 1290 Epoch 0: D Loss = 1.3850377798080444, G Loss = 0.7004989981651306 \n",
      "Iteration 1292 Epoch 0: D Loss = 1.3851144313812256, G Loss = 0.7004587650299072 \n",
      "Iteration 1294 Epoch 0: D Loss = 1.3851261138916016, G Loss = 0.7004451751708984 \n",
      "Iteration 1296 Epoch 0: D Loss = 1.3849472999572754, G Loss = 0.7003664970397949 \n",
      "Iteration 1298 Epoch 0: D Loss = 1.3851664066314697, G Loss = 0.7004318833351135 \n",
      "Iteration 1300 Epoch 0: D Loss = 1.38515043258667, G Loss = 0.7004954814910889 \n",
      "Iteration 1302 Epoch 0: D Loss = 1.3849635124206543, G Loss = 0.7004305124282837 \n",
      "Iteration 1304 Epoch 0: D Loss = 1.3848490715026855, G Loss = 0.7003958821296692 \n",
      "Iteration 1306 Epoch 0: D Loss = 1.3850767612457275, G Loss = 0.7003852128982544 \n",
      "Iteration 1308 Epoch 0: D Loss = 1.3850681781768799, G Loss = 0.7004871368408203 \n",
      "Iteration 1310 Epoch 0: D Loss = 1.3852397203445435, G Loss = 0.7004972696304321 \n",
      "Iteration 1312 Epoch 0: D Loss = 1.3848645687103271, G Loss = 0.7004987001419067 \n",
      "Iteration 1314 Epoch 0: D Loss = 1.3850901126861572, G Loss = 0.7004957795143127 \n",
      "Iteration 1316 Epoch 0: D Loss = 1.3850301504135132, G Loss = 0.700375497341156 \n",
      "Iteration 1318 Epoch 0: D Loss = 1.3848832845687866, G Loss = 0.7003908753395081 \n",
      "Iteration 1320 Epoch 0: D Loss = 1.3851008415222168, G Loss = 0.7002835273742676 \n",
      "Iteration 1322 Epoch 0: D Loss = 1.3851819038391113, G Loss = 0.7003715634346008 \n",
      "Iteration 1324 Epoch 0: D Loss = 1.384913682937622, G Loss = 0.7005140781402588 \n",
      "Iteration 1326 Epoch 0: D Loss = 1.3850600719451904, G Loss = 0.7004196047782898 \n",
      "Iteration 1328 Epoch 0: D Loss = 1.3850436210632324, G Loss = 0.7003804445266724 \n",
      "Iteration 1330 Epoch 0: D Loss = 1.3850690126419067, G Loss = 0.7004436254501343 \n",
      "Iteration 1332 Epoch 0: D Loss = 1.3851065635681152, G Loss = 0.7003917098045349 \n",
      "Iteration 1334 Epoch 0: D Loss = 1.3850576877593994, G Loss = 0.7003992199897766 \n",
      "Iteration 1336 Epoch 0: D Loss = 1.385040521621704, G Loss = 0.7004523873329163 \n",
      "Iteration 1338 Epoch 0: D Loss = 1.3849592208862305, G Loss = 0.7004930377006531 \n",
      "Iteration 1340 Epoch 0: D Loss = 1.384952425956726, G Loss = 0.700333297252655 \n",
      "Iteration 1342 Epoch 0: D Loss = 1.3849477767944336, G Loss = 0.7004873156547546 \n",
      "Iteration 1344 Epoch 0: D Loss = 1.3850433826446533, G Loss = 0.7004790306091309 \n",
      "Iteration 1346 Epoch 0: D Loss = 1.384956955909729, G Loss = 0.7004159092903137 \n",
      "Iteration 1348 Epoch 0: D Loss = 1.3850510120391846, G Loss = 0.7004093527793884 \n",
      "Iteration 1350 Epoch 0: D Loss = 1.3849934339523315, G Loss = 0.7004202604293823 \n",
      "Iteration 1352 Epoch 0: D Loss = 1.3849396705627441, G Loss = 0.7004826664924622 \n",
      "Iteration 1354 Epoch 0: D Loss = 1.3849742412567139, G Loss = 0.7005175352096558 \n",
      "Iteration 1356 Epoch 0: D Loss = 1.3849879503250122, G Loss = 0.7004714608192444 \n",
      "Iteration 1358 Epoch 0: D Loss = 1.3849234580993652, G Loss = 0.7003915309906006 \n",
      "Iteration 1360 Epoch 0: D Loss = 1.3849685192108154, G Loss = 0.7003615498542786 \n",
      "Iteration 1362 Epoch 0: D Loss = 1.3850549459457397, G Loss = 0.7004350423812866 \n",
      "Iteration 1364 Epoch 0: D Loss = 1.384892225265503, G Loss = 0.7003517746925354 \n",
      "Iteration 1366 Epoch 0: D Loss = 1.3850386142730713, G Loss = 0.7003517746925354 \n",
      "Iteration 1368 Epoch 0: D Loss = 1.3850972652435303, G Loss = 0.7003641724586487 \n",
      "Iteration 1370 Epoch 0: D Loss = 1.3849234580993652, G Loss = 0.7004808783531189 \n",
      "Iteration 1372 Epoch 0: D Loss = 1.38503897190094, G Loss = 0.7004191279411316 \n",
      "Iteration 1374 Epoch 0: D Loss = 1.385056734085083, G Loss = 0.7003682851791382 \n",
      "Iteration 1376 Epoch 0: D Loss = 1.3849895000457764, G Loss = 0.7004827857017517 \n",
      "Iteration 1378 Epoch 0: D Loss = 1.384817123413086, G Loss = 0.700419545173645 \n",
      "Iteration 1380 Epoch 0: D Loss = 1.385085105895996, G Loss = 0.7003695964813232 \n",
      "Iteration 1382 Epoch 0: D Loss = 1.3850321769714355, G Loss = 0.7004197835922241 \n",
      "Iteration 1384 Epoch 0: D Loss = 1.3849968910217285, G Loss = 0.7003129720687866 \n",
      "Iteration 1386 Epoch 0: D Loss = 1.3850066661834717, G Loss = 0.700474739074707 \n",
      "Iteration 1388 Epoch 0: D Loss = 1.3850476741790771, G Loss = 0.7003912925720215 \n",
      "Iteration 1390 Epoch 0: D Loss = 1.3850007057189941, G Loss = 0.7003217935562134 \n",
      "Iteration 1392 Epoch 0: D Loss = 1.38492751121521, G Loss = 0.7003154158592224 \n",
      "Iteration 1394 Epoch 0: D Loss = 1.385063886642456, G Loss = 0.700480580329895 \n",
      "Iteration 1396 Epoch 0: D Loss = 1.384911060333252, G Loss = 0.7003844976425171 \n",
      "Iteration 1398 Epoch 0: D Loss = 1.384976863861084, G Loss = 0.7003188729286194 \n",
      "Iteration 1400 Epoch 0: D Loss = 1.384945034980774, G Loss = 0.7003628015518188 \n",
      "Iteration 1402 Epoch 0: D Loss = 1.384943962097168, G Loss = 0.7003534436225891 \n",
      "Iteration 1404 Epoch 0: D Loss = 1.3850057125091553, G Loss = 0.7003427147865295 \n",
      "Iteration 1406 Epoch 0: D Loss = 1.3849430084228516, G Loss = 0.7003957033157349 \n",
      "Iteration 1408 Epoch 0: D Loss = 1.3849459886550903, G Loss = 0.7004075646400452 \n",
      "Iteration 1410 Epoch 0: D Loss = 1.3850698471069336, G Loss = 0.700413167476654 \n",
      "Iteration 1412 Epoch 0: D Loss = 1.384921908378601, G Loss = 0.7003250122070312 \n",
      "Iteration 1414 Epoch 0: D Loss = 1.3849925994873047, G Loss = 0.7002833485603333 \n",
      "Iteration 1416 Epoch 0: D Loss = 1.3849380016326904, G Loss = 0.7003912925720215 \n",
      "Iteration 1418 Epoch 0: D Loss = 1.3848729133605957, G Loss = 0.7003782987594604 \n",
      "Iteration 1420 Epoch 0: D Loss = 1.3849496841430664, G Loss = 0.7003620266914368 \n",
      "Iteration 1422 Epoch 0: D Loss = 1.3848514556884766, G Loss = 0.7002852559089661 \n",
      "Iteration 1424 Epoch 0: D Loss = 1.3848626613616943, G Loss = 0.7003940343856812 \n",
      "Iteration 1426 Epoch 0: D Loss = 1.3849889039993286, G Loss = 0.7004491090774536 \n",
      "Iteration 1428 Epoch 0: D Loss = 1.3848800659179688, G Loss = 0.7003437280654907 \n",
      "Iteration 1430 Epoch 0: D Loss = 1.3850163221359253, G Loss = 0.7004144787788391 \n",
      "Iteration 1432 Epoch 0: D Loss = 1.384981393814087, G Loss = 0.700385332107544 \n",
      "Iteration 1434 Epoch 0: D Loss = 1.38482666015625, G Loss = 0.7004770040512085 \n",
      "Iteration 1436 Epoch 0: D Loss = 1.3849766254425049, G Loss = 0.700401246547699 \n",
      "Iteration 1438 Epoch 0: D Loss = 1.384880542755127, G Loss = 0.7002944946289062 \n",
      "Iteration 1440 Epoch 0: D Loss = 1.3848745822906494, G Loss = 0.7003400325775146 \n",
      "Iteration 1442 Epoch 0: D Loss = 1.3850431442260742, G Loss = 0.7003597021102905 \n",
      "Iteration 1444 Epoch 0: D Loss = 1.3848885297775269, G Loss = 0.7003692388534546 \n",
      "Iteration 1446 Epoch 0: D Loss = 1.3848854303359985, G Loss = 0.7004085183143616 \n",
      "Iteration 1448 Epoch 0: D Loss = 1.385160207748413, G Loss = 0.7004106044769287 \n",
      "Iteration 1450 Epoch 0: D Loss = 1.38503897190094, G Loss = 0.7004604339599609 \n",
      "Iteration 1452 Epoch 0: D Loss = 1.3848949670791626, G Loss = 0.7003320455551147 \n",
      "Iteration 1454 Epoch 0: D Loss = 1.384901523590088, G Loss = 0.7002862095832825 \n",
      "Iteration 1456 Epoch 0: D Loss = 1.3848023414611816, G Loss = 0.7003967761993408 \n",
      "Iteration 1458 Epoch 0: D Loss = 1.3849139213562012, G Loss = 0.7003390789031982 \n",
      "Iteration 1460 Epoch 0: D Loss = 1.3848845958709717, G Loss = 0.7002909183502197 \n",
      "Iteration 1462 Epoch 0: D Loss = 1.3849066495895386, G Loss = 0.7003214359283447 \n",
      "Iteration 1464 Epoch 0: D Loss = 1.3848600387573242, G Loss = 0.7003783583641052 \n",
      "Iteration 1466 Epoch 0: D Loss = 1.384948968887329, G Loss = 0.7003300189971924 \n",
      "Iteration 1468 Epoch 0: D Loss = 1.3848466873168945, G Loss = 0.7003495693206787 \n",
      "Iteration 1470 Epoch 0: D Loss = 1.384978175163269, G Loss = 0.7003728151321411 \n",
      "Iteration 1472 Epoch 0: D Loss = 1.3848850727081299, G Loss = 0.7002975940704346 \n",
      "Iteration 1474 Epoch 0: D Loss = 1.384856104850769, G Loss = 0.7001837491989136 \n",
      "Iteration 1476 Epoch 0: D Loss = 1.384763479232788, G Loss = 0.7004297971725464 \n",
      "Iteration 1478 Epoch 0: D Loss = 1.3848955631256104, G Loss = 0.7002953290939331 \n",
      "Iteration 1480 Epoch 0: D Loss = 1.3849291801452637, G Loss = 0.7003226280212402 \n",
      "Iteration 1482 Epoch 0: D Loss = 1.3848742246627808, G Loss = 0.7003872394561768 \n",
      "Iteration 1484 Epoch 0: D Loss = 1.384814739227295, G Loss = 0.700370728969574 \n",
      "Iteration 1486 Epoch 0: D Loss = 1.3848612308502197, G Loss = 0.700284481048584 \n",
      "Iteration 1488 Epoch 0: D Loss = 1.3849270343780518, G Loss = 0.7002755403518677 \n",
      "Iteration 1490 Epoch 0: D Loss = 1.3848612308502197, G Loss = 0.7003430128097534 \n",
      "Iteration 1492 Epoch 0: D Loss = 1.38487708568573, G Loss = 0.7002996206283569 \n",
      "Iteration 1494 Epoch 0: D Loss = 1.3849623203277588, G Loss = 0.7002252340316772 \n",
      "Iteration 1496 Epoch 0: D Loss = 1.3848819732666016, G Loss = 0.7003698348999023 \n",
      "Iteration 1498 Epoch 0: D Loss = 1.3849060535430908, G Loss = 0.7002872824668884 \n",
      "Iteration 1500 Epoch 0: D Loss = 1.3849012851715088, G Loss = 0.7003099918365479 \n",
      "Iteration 1502 Epoch 0: D Loss = 1.3847112655639648, G Loss = 0.7004029750823975 \n",
      "Iteration 1504 Epoch 0: D Loss = 1.3847812414169312, G Loss = 0.700210452079773 \n",
      "Iteration 1506 Epoch 0: D Loss = 1.38493013381958, G Loss = 0.7003019452095032 \n",
      "Iteration 1508 Epoch 0: D Loss = 1.3849081993103027, G Loss = 0.7003740072250366 \n",
      "Iteration 1510 Epoch 0: D Loss = 1.3848159313201904, G Loss = 0.700219988822937 \n",
      "Iteration 1512 Epoch 0: D Loss = 1.3848953247070312, G Loss = 0.7002639770507812 \n",
      "Iteration 1514 Epoch 0: D Loss = 1.3848919868469238, G Loss = 0.7002360224723816 \n",
      "Iteration 1516 Epoch 0: D Loss = 1.3848061561584473, G Loss = 0.7003062963485718 \n",
      "Iteration 1518 Epoch 0: D Loss = 1.3850116729736328, G Loss = 0.7003694176673889 \n",
      "Iteration 1520 Epoch 0: D Loss = 1.3848888874053955, G Loss = 0.7002485394477844 \n",
      "Iteration 1522 Epoch 0: D Loss = 1.3849499225616455, G Loss = 0.7002893686294556 \n",
      "Iteration 1524 Epoch 0: D Loss = 1.3848035335540771, G Loss = 0.7004052996635437 \n",
      "Iteration 1526 Epoch 0: D Loss = 1.384886384010315, G Loss = 0.7001937031745911 \n",
      "Iteration 1528 Epoch 0: D Loss = 1.384758710861206, G Loss = 0.7003408670425415 \n",
      "Iteration 1530 Epoch 0: D Loss = 1.384823203086853, G Loss = 0.7003011703491211 \n",
      "Iteration 1532 Epoch 0: D Loss = 1.3847311735153198, G Loss = 0.7002357840538025 \n",
      "Iteration 1534 Epoch 0: D Loss = 1.3848397731781006, G Loss = 0.7002691626548767 \n",
      "Iteration 1536 Epoch 0: D Loss = 1.3848421573638916, G Loss = 0.7003166675567627 \n",
      "Iteration 1538 Epoch 0: D Loss = 1.3847591876983643, G Loss = 0.7002211213111877 \n",
      "Iteration 1540 Epoch 0: D Loss = 1.3848507404327393, G Loss = 0.7001965045928955 \n",
      "Iteration 1542 Epoch 0: D Loss = 1.3848117589950562, G Loss = 0.7003629207611084 \n",
      "Iteration 1544 Epoch 0: D Loss = 1.3846884965896606, G Loss = 0.7003024816513062 \n",
      "Iteration 1546 Epoch 0: D Loss = 1.3848516941070557, G Loss = 0.7002518773078918 \n",
      "Iteration 1548 Epoch 0: D Loss = 1.3848319053649902, G Loss = 0.7002240419387817 \n",
      "Iteration 1550 Epoch 0: D Loss = 1.3849103450775146, G Loss = 0.700248122215271 \n",
      "Iteration 1552 Epoch 0: D Loss = 1.3848849534988403, G Loss = 0.7003366947174072 \n",
      "Iteration 1554 Epoch 0: D Loss = 1.3847622871398926, G Loss = 0.7001660466194153 \n",
      "Iteration 1556 Epoch 0: D Loss = 1.3847787380218506, G Loss = 0.7002846598625183 \n",
      "Iteration 1558 Epoch 0: D Loss = 1.384673833847046, G Loss = 0.7002929449081421 \n",
      "Iteration 1560 Epoch 0: D Loss = 1.3848390579223633, G Loss = 0.7002089619636536 \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitHub\\GAN\\ImageGeneratorUNETandCIFAR100.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/GAN/ImageGeneratorUNETandCIFAR100.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m inputchk \u001b[39m=\u001b[39m discriminator(real_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/GAN/ImageGeneratorUNETandCIFAR100.ipynb#W3sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#print(inputchk.shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GitHub/GAN/ImageGeneratorUNETandCIFAR100.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m discriminator_loss_real \u001b[39m=\u001b[39m criterion_D(discriminator(real_images), real_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/GAN/ImageGeneratorUNETandCIFAR100.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m discriminator_loss_fake \u001b[39m=\u001b[39m criterion_D(discriminator(fake_images), fake_labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHub/GAN/ImageGeneratorUNETandCIFAR100.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m discriminator_loss \u001b[39m=\u001b[39m discriminator_loss_real \u001b[39m+\u001b[39m discriminator_loss_fake\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\torch\\nn\\functional.py:2884\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2882\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   2883\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 2884\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2885\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2886\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   2887\u001b[0m     )\n\u001b[0;32m   2889\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2890\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([32, 1])) that is different to the input size (torch.Size([16, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "print(\"debug chk\")\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(trainloader):\n",
    "        # Train the discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_images = images\n",
    "        #print(f\"size of real_images is {real_images.shape}\")        \n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        noise_input_D = torch.randn(batch_size, 3, 32,32)\n",
    "        fake_images = generator(noise_input_D).detach()\n",
    "        #print(f\"size of fake_images is {fake_images.shape}\")\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        inputchk = discriminator(real_images)\n",
    "        #print(inputchk.shape)\n",
    "        discriminator_loss_real = criterion_D(discriminator(real_images), real_labels)\n",
    "        discriminator_loss_fake = criterion_D(discriminator(fake_images), fake_labels)\n",
    "        discriminator_loss = discriminator_loss_real + discriminator_loss_fake\n",
    "        discriminator_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator\n",
    "        generator.zero_grad()\n",
    "        noise_input_G = torch.randn(batch_size, 3, 32,32)        \n",
    "        fake_images = generator(noise_input_G)\n",
    "        generator_loss = criterion_G(discriminator(fake_images), real_labels)\n",
    "        generator_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            print(f\"Iteration {i} Epoch {epoch}: D Loss = {discriminator_loss}, G Loss = {generator_loss} \")\n",
    "            \n",
    "        if abs(i-len(trainloader)) <= batch_size:\n",
    "            break          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), \"generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator.pth\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 3])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Generate some images\n",
    "with torch.no_grad():\n",
    "    noise_input_test = torch.randn(1, 3, 32,32)   \n",
    "    generated_images = generator(noise_input_test)\n",
    "    image_for_plot = generated_images.squeeze()\n",
    "    image_for_plot = image_for_plot.permute(1, 2, 0)\n",
    "print(image_for_plot.shape)\n",
    "plt.imshow(image_for_plot,cmap='gray')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ea948a86ffb9cb8476bbf8b2af39eb17488da6e229f09db40f40f089fac991a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
